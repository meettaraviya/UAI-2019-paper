\section{INTRODUCTION}
\label{sec:introduction}

Markov Decision Problems (MDPs)~\cite{Bellman:1957,Puterman:1994} are an abstraction of sequential decision making, providing a formal basis for problems such as automated planning and reinforcement learning. Applications of MDPs span a variety of domains~\cite{White:1985,White:1988,Feinberg+Schwarz:2002}.

An MDP describes an agent's \textit{environment}. For each state $s \in S$ in which the agent can be, and for each action $a \in A$ that it can take, the MDP specifies for all $s^{\prime} \in S$ the probability that taking $a$ from $s$ will reach $s^{\prime}$. Denote this probability$ P(s, a, s^{\prime})$. The transition from $s$ to $s^{\prime}$ by taking action $a$ also yields a numeric reward $R(s, a, s^{\prime})$. %\highlight{Here, $S$ and $A$ are finite sets.}

The agent's behaviour is encoded as a \textit{policy} $\pi: S \to A$, which specifies the action $\pi(s)$ that the agent must take from each state $s \in S$. If indeed the agent starts from some state $s^{0} \in S$ and follows $\pi$, then it encounters a random ``state-reward'' sequence $s^{0}, r^{0}, s^{1}, r^{1}, \dots$ over time, wherein for $t \geq 0$, $s^{t + 1}$ is drawn according to $P(s^{t}, \pi(s^{t}), \cdot)$, and $r^{t} = R(s^{t}, \pi(s^{t}), s^{t + 1})$. The \textit{value} of $s^{0}$ is commonly defined to be $\mathbb{E}[r^{0} + \gamma r^{1} + \gamma^{2} r^{2} + \dots]$, where $\gamma \in [0, 1)$ is a discount factor. An MDP is fully specified by $S$, $A$, $P$, $R$, and $\gamma$. In this paper, we shall assume that $S$ and $A$ are both finite.

Our definition above interprets value as ``infinite discounted reward''. The analysis provided in our paper can also be applied if alternative definitions such as ``average reward''~\cite{Mahadevan:1996}
and ``total reward''~\cite{Fearnley:2010} are used. Also, it suffices for our purposes to consider policies that do not vary over time, and which map states to actions (rather than map histories to distributions over actions). In other words, we consider policies that are stationary, deterministic, and Markovian.

%\highlight{\footnote{\highlight{We only consider stationary deterministic Markovian policies}}} 


Let $(S, A, P, R, \gamma)$ be an arbitrary MDP, and $\Pi$ be the set of all policies for this MDP. It is a well-known result that there is an \textit{optimal policy} $\pi^{\star} \in \Pi$ such that for all $\pi \in \Pi$, $s \in S$, $V^{\pi^{\star}}(s) \geq V^{\pi}(s)$. The problem we consider in this paper is precisely that of computing an optimal policy for a given MDP. We assume that $P$ and $R$ are provided as tables; there is no need for sampling (as is typical in reinforcement learning~\cite{Sutton+Barto:1998}) or for any sort of generalisation (as is needed when $S$ or $A$ are very large~\cite{Bertsekas+Tsitsiklis:1996}). Over the decades, many families of algorithms have been proposed to solve
the problem we consider, which is denoted MDP planning. Approaches vary from formulations using Linear Programming to dynamic programming techniques such as Value Iteration, Policy Iteration, and combinations~\cite{Littman+DK:1995}.

In this paper, we consider the Policy Iteration (PI) family of algorithms~\cite{Howard:1960}. PI is a conceptually-simple, iterative approach to search the space of policies. Although experimenters tend to find PI to work very well in practice~\cite[see Section 4.2]{Littman+DK:1995}, there has only been limited progress towards formally quantifying its running time. We restrict our focus to \textit{strong} running-time bounds---those that depend only on the number of states and actions in the given MDP. Strong bounds have long held appeal from a theoretical standpoint~\cite{Megiddo:1982}. Suppose we assume that arithmetic, relational and logical operations can all be performed exactly, in constant time, regardless of the operand size, the question is how many operations are needed to compute an optimal policy. The number of operations performed by PI is known to be polynomial in the number of states and actions if associated parameters such as the discount rate $\gamma$ and the number of bits $B$ needed to represent the MDP are treated as constants~\cite{Ye:2011,Scherrer:2013}. Strong bounds, on the other hand, have no dependence on parameters such as $\gamma$ and $B$.

The first non-trivial strong bounds for PI were provided by Mansour and Singh~\shortcite{Mansour+Singh:1999}, who showed an improved upper bound on the running time of Howard's PI (a commonly-used variant), and proposed a randomised variant of PI with a tighter upper bound. More recently, even tighter strong upper bounds have been shown for newer variants of PI---both deterministic~\cite{Kalyanakrishnan+Gupta} and randomised~\cite{Kalyanakrishnan+MG:2016}.

Our main contributions are based on the algorithms and analysis of Mansour and Singh~\shortcite{Mansour+Singh:1999}. Applying some fresh ideas in conjunction with their proof structure, we show significantly tighter upper bounds for related randomised algorithms, including a variant of Howard's PI, on $k$-action MDPs, $k \geq 3$. We provide a \textit{lower} bound for the randomised variant of Mansour and Singh~\shortcite{Mansour+Singh:1999}, matching a bound shown previously for Howard's PI. We also investigate a randomised ``batch-switching'' variant of PI~\cite{Kalyanakrishnan+MG-bspi:2016}. While the (strong) upper bound we furnish for this variant is the tightest yet for the PI family, experiments suggest that the randomised algorithm of Mansour and Singh~\shortcite{Mansour+Singh:1999} might itself be more efficient.

We present our technical contributions in sections \ref{sec:upperbounds} and \ref{sec:lowerbound}, after first describing PI in Section~\ref{sec:policy_iteration} and surveying previous analyses in Section~\ref{sec:relatedworkandcontribution}. In Section~\ref{sec:experiments} we share experimental findings to accompany our theoretical results. We conclude with a discussion in Section~\ref{sec:conclusion}.



